{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Review the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We review and prepare last phase dataset to processing and learning so we import pandas to convert Dataset.csv file to dataframe. They are in csv format and separated by ',' delimeter. we will use read_csv function to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Message</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>سلام\\r\\n عیدی شما آماده است. عدد 1 را به شماره...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>رفاه گیلان\\r\\nچای تشریفات47%\\r\\nروغن لادن15%\\r...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>درست و اصولی لاغر  شوید\\r\\n*غیرحضوری*\\r\\n\\r\\nک...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>خبرهای هیجان انگیز و جنجالی برای علاقمندان به ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>هدیه ویژه نوروزی برای تمام مشترکین سرویس تلگرا...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Message   Tag\n",
       "0           0  سلام\\r\\n عیدی شما آماده است. عدد 1 را به شماره...  Spam\n",
       "1           1  رفاه گیلان\\r\\nچای تشریفات47%\\r\\nروغن لادن15%\\r...  Spam\n",
       "2           2  درست و اصولی لاغر  شوید\\r\\n*غیرحضوری*\\r\\n\\r\\nک...  Spam\n",
       "3           3  خبرهای هیجان انگیز و جنجالی برای علاقمندان به ...  Spam\n",
       "4           4  هدیه ویژه نوروزی برای تمام مشترکین سرویس تلگرا...  Spam"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has three columns that we need second and third column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 416 entries, 0 to 415\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    416 non-null int64\n",
      "Message       416 non-null object\n",
      "Tag           416 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we got 416 samples that they're tagged with Spam and Non Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam        236\n",
       "Non Spam    180\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam messages are more than non spams so it is expected to detect spam messages better than non spams.\n",
    "We will save messages daa series to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_messages = df['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of using scikit learn and ensuring models compatibility we need to encode class labels\n",
    "we will use sklearn.preprocessing.LabelEncoder to encode all tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df['Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare our stop words list for preprcessing step that is next step.\n",
    "Persian stop words list is sotred in file named stop-words.txt and the words are listed line by line.\n",
    "Using pandas we will make a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اتفاقا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>احتراما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>احتمالا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آري</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0   اتفاقا\n",
       "1  احتراما\n",
       "2  احتمالا\n",
       "3      اري\n",
       "4      آري"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = pd.read_csv('stop-words.txt', encoding='utf-8', delimiter='\\n', header=None)\n",
    "stop_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Text Preprocessing\n",
    "We are going to words as features (n-gram language model) and counting their occurance. If we perform this strategy we got lot's of features that many of them is not useful. The classifier would takes to long time to train and likely overfit. so we will do following preprocessing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Lots of spam SMS contains phone numbers, urls or even email addresses so we will use Regex to convert all of them to a key word.<br>\n",
    "<ul>\n",
    "    <li>Replace <b>phone numbers</b> with <code>'شماره_تلفن'</code></li>\n",
    "    <li>Replaec <b>URLs</b> with <code>آدرس _ لینک</code></li>\n",
    "    <li>Replaec <b>email</b> with <code>آدرس _ایمیل</code></li>\n",
    "</ul><br>\n",
    "But to use regex first we should transfer all persian numbers to english that we use a function named <code>numbers_to_english()</code><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    text = numbers_to_english(text)\n",
    "    text = re.sub(email_regex, 'آدرس_ایمیل', text)\n",
    "    text = re.sub(phone_regex, 'شماره_تلفن', text)\n",
    "    text = re.sub(url_regex, 'آدرس_لینک', text)\n",
    "    text = re.sub(number_regex, 'عدد_رقم', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hazm to normalize text and special characters<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "For persian stemming there is Stemmer class in Hazm that can stem all words <br>\n",
    "Stemmer will find words stem for example it will replace 'کتاب‌ها' with 'کتاب'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "stemmer = Stemmer()\n",
    "for index, term in enumerate(tokens):\n",
    "    tokens[index] = stemmer.stem(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Some words in Persian language while necessary, don't contribute much meaning of phrase. These words, such as 'از', 'احتراما' are called <b>stop words</b>. They can effects on results and should be filtered out.<br>\n",
    "<div style=\"border:1px solid #cfcfcf;border-radius: 2px;background: #f7f7f7;line-height: 1.21429em;padding:4px;\">\n",
    "text = ' '.join(term for term in tokens if term not in stop_words.values)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the final preprocessing step will have following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_regex = \"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\"\n",
    "phone_regex = \"((\\+98|0)?9\\d{9})|(0\\d{2}\\d{8}|\\d{8})\"\n",
    "number_regex = \"[1-9]\\d+\"\n",
    "url_regex = \"(@^(https?|ftp)://[^\\s/$.?#].[^\\s]*$@iS)|(t.me/[a-z|0-9]{4,})\"\\\n",
    "    \"|((https?://)?(w{3}.)?[a-zA-Z0-9]+.[a-zA-Z]{2,}(/[a-zA-Z0-9]*)*)\"\n",
    "\n",
    "\n",
    "def numbers_to_english(text):\n",
    "    text = text.replace('۰', '0')\n",
    "    text = text.replace('۱', '1')\n",
    "    text = text.replace('۲', '2')\n",
    "    text = text.replace('۳', '3')\n",
    "    text = text.replace('۴', '4')\n",
    "    text = text.replace('۵', '5')\n",
    "    text = text.replace('۶', '6')\n",
    "    text = text.replace('۷', '7')\n",
    "    text = text.replace('۸', '8')\n",
    "    text = text.replace('۹', '9')\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = numbers_to_english(text)\n",
    "    text = re.sub(email_regex, 'آدرس_ایمیل', text)\n",
    "    text = re.sub(phone_regex, 'شماره_تلفن', text)\n",
    "    text = re.sub(url_regex, 'آدرس_لینک', text)\n",
    "    text = re.sub(number_regex, 'عدد_رقم', text)\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    tokens = word_tokenize(text)    \n",
    "    stemmer = Stemmer()\n",
    "    for index, term in enumerate(tokens):\n",
    "        tokens[index] = stemmer.stem(term)\n",
    "    text = ' '.join(term for term in tokens if term not in stop_words.values)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we preprocess all messages and save them inside of an array named documents to start learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سلا عید آماده اس . عدد ۱ شماره عدد_رق پیامک کنید عضو اپلیکیشن بازیانا دسترس نامحدود بازی پرتال عدد_رق شارژ عید بگیرید . \"',\n",
       " 'رفاه گیل چا تشریفاتعدد_رقم٪ روغن لادنعدد_رقم٪ اسپاگت ماناعدد_رقم٪ ۱ /عدد_رق لغوعدد ۱',\n",
       " 'درس اصول لاغر شوید *غیرحضوری* کلیک کنید : آدرس_لینک',\n",
       " 'خبر هیج انگیز جنجال برا علاقمند فیل سریال ، هنرمند خبر حاشیه . عضو اپلیکیشن مواستار قرعه کش کمپین بهار عدد_رق جوایز پژو عدد_رق ، آیفون x سامسونگ s ۹ شرک کن جایزه ببر ارسال عدد : عدد_رق دانلود اپلیکیشن : آدرس_لینک',\n",
       " 'هدیه ویژه نوروز برا تما مشترکین سرویس تلگراف . مناسب نوروز عدد_رق ، کتابچه ارز عدد_رق تمام کاربران که ارسال ۱ عدد_رق عضو سرویس کتابخانه الکترونیک تلگراف تعلق خواهد_گرف .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for content in raw_messages:\n",
    "    content = preprocessing_text(content)\n",
    "    documents.append(content)\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Features\n",
    "Now we've prepared the dataset for meaningful terms we're ready to construct features. So we will start will with tokenizing terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We will tokenize individual terms and generating a <b>bag of words</b> model. But this model have a weakness that it fails to capture innate structure of human language and only represent occurence of terms.<br>\n",
    "Alternatively we can use <b>n-gram</b> model to preserve words order and acn capture more information than bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the tf-idf statistic\n",
    "The next step is assign each n-gram a feature and then compute the n=gram's frequency using some statistic.<br>\n",
    "<br>\n",
    "One good way to do is <b>tf-idf</b>. <b>term frequency (tf)</b> counts each n-gram occurance in a document to weight it's importance. But it won't work much good in some cases because of weighting common words that are in every document much more. Therefore to solve it we'll downweight term frequency with <b>inverse document frequency (idf)</b>, which is calculated by logarithmically scaling the inverse of the fraction of training examples that contain a given term. By combining these two statistic formulas the tf-idf statistics:<br>\n",
    "$$ tf-idf(t,i) = tf(t,i)\\times idf(t) $$ <br>\n",
    "$$ =tf(t,i) \\times \\log \\left( \\frac{M}{m_t} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $tf(t,i)$ is the term frequency for term $t$ in the $i$th training example, $M$ is the total number of training examples, and $m_t$ is the number of training examples that contain the term $t$.<br><br>\n",
    "Scikit Learn has a class called <code>TfidfVectorizer</code> that perform n-gram tokenization and also computes the tf-idf statistic.<br>\n",
    "According to it's documentation it will do two things:<br>\n",
    "<ol>\n",
    "    <li>Computing tf-idf and avoiding devision by zero using <b>smoothing</b> (laplace) </li>\n",
    "    <li>L2 normalization using <b>Euclidean</b> norm </li>\n",
    "</ol><br>\n",
    "Finally we're ready to transform a corpus of text data into a matrix of numbers with one row per training sample and one column per $n$-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "n_grams = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dimensions of the <code>n_grams</code> matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 3834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's looks like the tokenizer extract 3878 unigrams and bigrams.Since each training set use only a few of these unigram and bigrams this matrix consists of zeros and is called a <b>sparse matrix</b>. But <code>TfidfVectorizer</code> handle it using Scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Training and evaluating model\n",
    "These all was preparing work and we haven't done any learning algorithm yet. This step we'll train a model using machine learning algorithms. We'll use a classifier called <b>Support Vector Machine (SVM)</b>. It is a good classifier for binary classification and attemps to find best planes that separates two classes.<br>\n",
    "I've selected SVM with <b>linear kernel</b> beacause of following reasons:<br>\n",
    "<ul>\n",
    "    <li style=\"margin:6px 0\">Text is often lineary separable</li>\n",
    "    <li style=\"margin:6px 0\">Text has a lot of features</li>\n",
    "    <i>The linear kernel is good when there is a lot of features. That's because mapping the data to a higher dimensional space does not really improve the performance. In text classification, both the numbers of instances (document) and features (words) are large. The decision boundary produced by a RBF kernel when the data is linearly separable is almost the same as the decision boundary produced by a linear kernel. Mapping data to a higher dimensional space using an RBF kernel is not useful.</i>\n",
    "    <li style=\"margin:6px 0\">Linear kernel is faster</li>\n",
    "    <li style=\"margin:6px 0\">Less parameters to optimize</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Analysis\n",
    "First we have to find out how good is SVM on the dataset, so we start by <b>Hold-Out</b> method: an 80/20 training and test set split. We will measure $F_1$ score to balance precision and recall as metrics. We will use <b>hinge loss</b> function to train classifier.<br>\n",
    "<br>$$ F_1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        n_grams,\n",
    "        labels,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=labels\n",
    "    )\n",
    "clf = svm.LinearSVC(loss='hinge', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run cross validation to guarantee whether this performance is consistant. Let's take a look at <b>confusion matrix</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>non spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>spam</th>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non spam</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted         \n",
       "                     spam non spam\n",
       "actual spam            51        3\n",
       "       non spam         1       70"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    metrics.confusion_matrix(y_test, y_pred),\n",
    "    index=[['actual', 'actual'], ['spam', 'non spam']],\n",
    "    columns=[['predicted', 'predicted'], ['spam', 'non spam']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier make a mistake likely when message is spam, it's typically <b>False Negative</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_space = np.linspace(100, len(raw_messages) * 0.8, 10, dtype='int')\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    estimator=svm.LinearSVC(loss='hinge', C=1e10),\n",
    "    X=n_grams,\n",
    "    y=labels,\n",
    "    train_sizes=samples_space,\n",
    "    cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=40),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tidy(sample_space, train_scores, valid_scores):\n",
    "    messy_format = pd.DataFrame(\n",
    "        np.stack((samples_space, train_scores.mean(axis=1),\n",
    "                  valid_scores.mean(axis=1)), axis=1),\n",
    "        columns=['# of training examples', 'Training set', 'Validation set']\n",
    "    )\n",
    "    \n",
    "    return pd.melt(\n",
    "        messy_format,\n",
    "        id_vars='# of training examples',\n",
    "        value_vars=['Training set', 'Validation set'],\n",
    "        var_name='Scores',\n",
    "        value_name='F1 score'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "g = sns.FacetGrid(\n",
    "    make_tidy(samples_space, train_scores, valid_scores), hue='Scores', size=5\n",
    ")\n",
    "\n",
    "g.map(plt.scatter, '# of training examples', 'F1 score')\n",
    "g.map(plt.plot, '# of training examples', 'F1 score').add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model with Cross Validation\n",
    "Cross-validation is a technique for evaluating machine learning models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.LinearSVC(loss='hinge', C=1)\n",
    "scores = cross_val_score(classifier,\n",
    "        n_grams,\n",
    "        labels,\n",
    "        cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2),\n",
    "        scoring='f1')\n",
    "\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got mean score 97% and 93% confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What terms are the top predictors of spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آدرس_لینک     2.993823\n",
      "شماره_تلفن    2.633741\n",
      "عدد_رق        1.879351\n",
      "ارسال         1.811004\n",
      "ویژه          1.198835\n",
      "تخفیف         1.071500\n",
      "رایگ          1.052526\n",
      "شارژ          1.035939\n",
      "روز           0.964954\n",
      "ایرانسل       0.943415\n",
      "خرید          0.936982\n",
      "فر            0.865509\n",
      "تماس          0.852475\n",
      "ایر           0.823884\n",
      "هدیه          0.817125\n",
      "شماره         0.804614\n",
      "کش            0.791897\n",
      "ستاره         0.780390\n",
      "همراه         0.771419\n",
      "قرعه          0.753615\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clf.fit(n_grams, labels)\n",
    "common_spams = pd.Series(\n",
    "    clf.coef_.T.ravel(),                 \n",
    "    index=vectorizer.get_feature_names()\n",
    ").sort_values(ascending=False)[:20]\n",
    "print(common_spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
